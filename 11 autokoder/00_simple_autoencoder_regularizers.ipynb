{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Inspirations:\n",
        "\n",
        "https://keras.io/examples/vision/autoencoder/\n",
        "\n",
        "https://blog.keras.io/building-autoencoders-in-keras.html"
      ],
      "metadata": {
        "id": "NZ2XXvtLOHjt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "iFsvbvmHOhM3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're using MNIST digits, and we're discarding the labels (since we're only interested in encoding/decoding the input images).\n",
        "\n"
      ],
      "metadata": {
        "id": "bwLp4xs6ObGH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import mnist\n",
        "import numpy as np\n",
        "(x_train, _), (x_test, y_test) = mnist.load_data()"
      ],
      "metadata": {
        "id": "nGn0PTY2OcNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will normalize all values between 0 and 1 and we will flatten the 28x28 images into vectors of size 784."
      ],
      "metadata": {
        "id": "Zrzvn7wBOdQr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = x_train.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
        "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)"
      ],
      "metadata": {
        "id": "2ALXE8lhOe5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "HH10yUzLOfTv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll start simple, with a single fully-connected neural layer as encoder and as decoder:"
      ],
      "metadata": {
        "id": "LUkxmLACN30B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kc1MQU4fNoaG"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras import layers\n",
        "from keras import regularizers\n",
        "\n",
        "# This is the size of our encoded representations\n",
        "encoding_dim = 32  # 32 floats -> compression of factor 784/32=24.5, assuming the input is 784 floats\n",
        "# This is our input image\n",
        "input_img = keras.Input(shape=(784,))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# \"encoded\" is the encoded representation of the input\n",
        "encoded = layers.Dense(encoding_dim, activation='relu')(input_img)\n",
        "# encoded = layers.Dense(encoding_dim, activation='relu', activity_regularizer=regularizers.l1(10e-5))(input_img)\n",
        "# \"decoded\" is the lossy reconstruction of the input\n",
        "decoded = layers.Dense(784, activation='sigmoid')(encoded)"
      ],
      "metadata": {
        "id": "ZNOdMwfJU4XR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's also create a separate autoencoder and encoder model:"
      ],
      "metadata": {
        "id": "TX0U7oiSN-mN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This model maps an input to its reconstruction\n",
        "autoencoder = keras.Model(input_img, decoded)"
      ],
      "metadata": {
        "id": "XV62Wdh2VryQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This model maps an input to its encoded representation\n",
        "encoder = keras.Model(input_img, encoded)"
      ],
      "metadata": {
        "id": "R1TKsa_rN_4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As well as the decoder model:"
      ],
      "metadata": {
        "id": "PX5V1br5OGhL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is our encoded (32-dimensional) input\n",
        "encoded_input = keras.Input(shape=(encoding_dim,))\n",
        "# Retrieve the last layer of the autoencoder model\n",
        "decoder_layer = autoencoder.layers[-1]\n",
        "# Create the decoder model\n",
        "decoder = keras.Model(encoded_input, decoder_layer(encoded_input))"
      ],
      "metadata": {
        "id": "irHNbGStOAdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')"
      ],
      "metadata": {
        "id": "N-oNvp7YOXV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder.fit(x_train, x_train,\n",
        "                epochs=50,\n",
        "                batch_size=256,\n",
        "                shuffle=True,\n",
        "                validation_data=(x_test, x_test))"
      ],
      "metadata": {
        "id": "9BIWJGAnOrup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " We can visualize the reconstructed inputs and the encoded representations."
      ],
      "metadata": {
        "id": "DA8quEf8Ou52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode and decode some digits\n",
        "# Note that we take them from the *test* set\n",
        "encoded_imgs = encoder.predict(x_test)\n",
        "decoded_imgs = decoder.predict(encoded_imgs)"
      ],
      "metadata": {
        "id": "FWQyoo_rOZDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use Matplotlib (don't ask)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "n = 10  # How many digits we will display\n",
        "plt.figure(figsize=(20, 4))\n",
        "for i in range(n):\n",
        "    # Display original\n",
        "    ax = plt.subplot(2, n, i + 1)\n",
        "    plt.imshow(x_test[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    # Display reconstruction\n",
        "    ax = plt.subplot(2, n, i + 1 + n)\n",
        "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WUwscX_HOyMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The top row is the original digits, and the bottom row is the reconstructed digits."
      ],
      "metadata": {
        "id": "IBJKAWOzOzta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the sparsity of the encoded representations. encoded_imgs.mean()\n",
        "encoded_imgs.mean()"
      ],
      "metadata": {
        "id": "FDraheH6SFZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "TuStFojNO14a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "# Użyj PCA do wizualizacji klastrów\n",
        "pca = PCA(n_components=2)  # Redukcja wymiarów za pomocą PCA do 2 komponentów\n",
        "encoded_imgs_2d = pca.fit_transform(encoded_imgs)  # Przekształcenie zakodowanych wektorów do 2D"
      ],
      "metadata": {
        "id": "nK9vfzSMamys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "# Użyj KMeans do analizy klastrów\n",
        "kmeans = KMeans(n_clusters=10, random_state=0)  # Algorytm KMeans z 10 klastrami\n",
        "clusters = kmeans.fit_predict(encoded_imgs)     # Dopasowanie modelu KMeans i przypisanie klastrów"
      ],
      "metadata": {
        "id": "y0KP7a_gan4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Wizualizacja klastrów\n",
        "plt.figure(figsize=(10, 8))  # Ustawienie rozmiaru wykresu\n",
        "plt.scatter(encoded_imgs_2d[:, 0], encoded_imgs_2d[:, 1], c=clusters, cmap='tab10')  # Wykres rozproszenia z kolorami odpowiadającymi klastrom\n",
        "plt.colorbar()  # Dodanie paska kolorów\n",
        "plt.show()  # Wyświetlenie wykresu"
      ],
      "metadata": {
        "id": "ebHzLbjeaoee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mapowanie klastrów na oryginalne etykiety\n",
        "def map_clusters_to_labels(clusters, labels):\n",
        "    label_map = {}  # Słownik do mapowania klastrów na etykiety\n",
        "    for cluster in np.unique(clusters):  # Iteracja po unikalnych klastrach\n",
        "        indices = np.where(clusters == cluster)  # Znalezienie indeksów próbek należących do danego klastra\n",
        "        true_labels = labels[indices]  # Pobranie prawdziwych etykiet dla tych próbek\n",
        "        most_common_label = np.bincount(true_labels).argmax()  # Znalezienie najczęściej występującej etykiety\n",
        "        label_map[cluster] = most_common_label  # Przypisanie najczęstszej etykiety do klastra\n",
        "    print(f\"Przypisanie najczęstszej etykiety do klastra {label_map}\")\n",
        "    return np.vectorize(label_map.get)(clusters)  # Mapowanie klastrów na etykiety"
      ],
      "metadata": {
        "id": "jyim4gTfapBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "predicted_labels = map_clusters_to_labels(clusters, y_test)  # Mapowanie klastrów na przewidywane etykiety\n",
        "accuracy = accuracy_score(y_test, predicted_labels)  # Obliczenie dokładności klastrowania\n",
        "print(f\"accuracy={accuracy}\")"
      ],
      "metadata": {
        "id": "WqgPdpieatHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Wyświetlenie macierzy konfuzji\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test, predicted_labels)  # Macierz konfuzji dla prawdziwych i przewidywanych etykiet\n",
        "plt.figure(figsize=(10, 8))  # Ustawienie rozmiaru wykresu\n",
        "import seaborn as sns\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap='Blues')  # Wykres macierzy konfuzji\n",
        "plt.xlabel('Predicted')  # Etykieta osi x\n",
        "plt.ylabel('True')  # Etykieta osi y\n",
        "plt.show()  # Wyświetlenie wykresu"
      ],
      "metadata": {
        "id": "MHVzWwCqatcs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}