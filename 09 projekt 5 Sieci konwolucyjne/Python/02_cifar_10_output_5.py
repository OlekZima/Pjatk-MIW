# -*- coding: utf-8 -*-
"""02_cifar 10_output_5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eaOI5iNhXPcgb_VU4MsEaTSIpAzT5xDa
"""

import numpy as np
import keras
import matplotlib.pyplot as plt
from keras.models import Model
from keras.layers import Input, Dense, Flatten, Conv2D, MaxPooling2D, Dropout
from keras.utils import plot_model
from keras.datasets import cifar10

# Definicja etykiet klas CIFAR-10
labels = ["Samolot", "Samochód", "Ptak", "Kot", "Jeleń", "Pies", "Żaba", "Koń", "Statek", "Ciężarówka"]

# Załaduj dane CIFAR-10
(x_train, y_train), (x_test, y_test) = cifar10.load_data()
# podgląd
print("etykieta ",labels[y_train[0,0]])
plt.imshow(x_train[0])

# Przetwórz dane
# Dane typu float32, modele często lepiej radzą sobie z danymi liczbowymi w formie zmiennoprzecinkowej.
# Normalizujemy dane do przedziału [0, 1]
x_train = x_train.astype('float32') / 255
x_test = x_test.astype('float32') / 255

# Konwertuj etykiety na kategorie
# kodowanie kategoryczne (one-hot encoding) na zbiorze etykiet treningowych
# zamienia etykiety klas na postać binarną w formie wektorów zer i jedynek
# 3 -> [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]
num_classes = 10
y_train = keras.utils.to_categorical(y_train, num_classes)
y_test = keras.utils.to_categorical(y_test, num_classes)

# Zdefiniuj model za pomocą Functional API
# Conv2D(liczba filtrów,...)
input_shape = (32, 32, 3)  # Wymiary obrazów CIFAR-10
inputs = Input(shape=input_shape)
x = Conv2D(32, kernel_size=(3, 3), padding='same',activation='relu')(inputs)
x = Conv2D(32, kernel_size=(3, 3), padding='same',activation='relu')(x)
x = MaxPooling2D(pool_size=(2, 2))(x)
x = Dropout(0.25)(x)
x = Conv2D(64, kernel_size=(3, 3), padding='same',activation='relu')(x)
x = Conv2D(64, kernel_size=(3, 3), padding='same',activation='relu')(x)
x = MaxPooling2D(pool_size=(2, 2))(x)
x = Dropout(0.25)(x)
x = Flatten()(x)
x = Dense(512, activation='relu')(x)
x = Dropout(0.5)(x)
outputs = Dense(num_classes, activation='softmax')(x)
model = Model(inputs=inputs, outputs=outputs)

# Skompiluj model
# 'categorical_crossentropy' średnia z logarytmu przewidywanych prawdopodobieństw dla prawdziwej klasy
# Optymalizator Adam (Adaptive Moment Estimation) wykorzystuje adaptacyjne momenty gradientu do efektywnego dostosowywania wag modelu podczas treningu
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
# Trenuj model
# batch_size liczba próbek treningowych użytych do jednej aktualizacji wag modelu podczas jednej iteracji treningowej
model.fit(x_train, y_train, batch_size=128, epochs=20, verbose='auto', validation_data=(x_test, y_test))

# Ocena modelu
loss, accuracy = model.evaluate(x_test, y_test, verbose='auto')
print('Test loss:', loss)
print('Test accuracy:', accuracy)

# Wizualizacja modelu
model.summary()
plot_model(model, show_shapes=True, show_layer_names=True)# to_file='model_plot.png',

# Wyświetlanie przykładów źle sklasyfikowanych
predictions = np.argmax(model.predict(x_test), axis=1);
y_test_flat = np.argmax(y_test, axis=1);
incorrect_indices = np.nonzero(predictions != y_test_flat)[0]

for i in range(5):
    idx = incorrect_indices[i]
    print("Przykład źle sklasyfikowany nr", i+1)
    plt.imshow(x_test[idx])
    plt.xlabel(f"True label:  {labels[y_test_flat[idx]]}, Predicted label:  {labels[predictions[idx]]}")
    plt.show()

# Visualisation
from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix
cm = confusion_matrix(y_test_flat, predictions)
disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=labels)
fig, ax = plt.subplots(figsize=(10, 10))
disp = disp.plot(xticks_rotation='vertical', ax=ax,cmap='summer')
plt.show()